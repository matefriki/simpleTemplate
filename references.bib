% BOOKS

@book{Salsa,
  title={Partial Differential Equations in Action: From Modelling to Theory},
  author={Sandro Salsa},
  isbn={9788847007529},
  lccn={2007938891},
  series={Universitext},
  url={https://books.google.es/books?id=ZO4y7wl4pngC},
  year={2008},
  publisher={Springer Milan}
}

@book{DeepLearningBook,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

% CBMM Memos

@article {Memo58,
	title = {Why and When Can Deep - but Not Shallow - Networks Avoid the Curse of Dimensionality: a Review},
	year = {2016},
	month = {11/2016},
	abstract = {<p>The paper reviews and extends an emerging body of theoretical results on deep learning including the conditions under which it can be exponentially better than shallow learning. A class of deep convolutional networks represent an important special case of these conditions, though weight sharing is not the main reason for their exponential advantage. Implications of a few key theorems are discussed, together with new results, open problems and conjectures.</p>
},
	author = {Tomaso Poggio and Hrushikesh Mhaskar and Lorenzo Rosasco and Brando Miranda and Qianli Liao},
	note = "\url{http://cbmm.mit.edu/publications/why-and-when-can-deep-not-shallow-networks-avoid-curse-dimensionality-review}"
}

@article {Memo45,
	title = {Learning Functions: When Is Deep Better Than Shallow},
	year = {2016},
	abstract = {<p>While the universal approximation property holds both for hierarchical and shallow networks, we prove that deep (hierarchical) networks can approximate the class of compositional functions with the same accuracy as shallow networks but with exponentially lower number of training parameters as well as VC-dimension. This theorem settles an old conjecture by Bengio on the role of depth in networks. We then define a general class of scalable, shift-invariant algorithms to show a simple and natural set of requirements that justify deep convolutional networks.</p>
},
	url = {https://arxiv.org/pdf/1603.00988v4.pdf},
	author = {Hrushikesh Mhaskar and Qianli Liao and Tomaso Poggio}
}

@article {Memo35,
	title = {Deep Convolutional Networks are Hierarchical Kernel Machines},
	number = {035},
	year = {2015},
	month = {06/17/2015},
	abstract = {<p>We extend i-theory to incorporate not only pooling but also rectifying nonlinearities in an extended HW module (eHW) designed for supervised learning. The two operations roughly correspond to invariance and selectivity, respectively. Under the assumption of normalized inputs, we show that appropriate linear combinations of rectifying nonlinearities are equivalent to radial kernels. If pooling is present an equivalent kernel also exist. Thus present-day DCNs (Deep Convolutional Networks) can be exactly equivalent to a hierarchy of kernel machines with pooling and non-pooling layers. Finally, we describe a conjecture for theoretically understanding hierarchies of such modules. A main consequence of the conjecture is that hierarchies of eHW modules minimize memory requirements while computing a selective and invariant representation.</p>
},
	author = {F. Anselmi and Lorenzo Rosasco and Cheston Tan and Tomaso Poggio}
}

% FORGETTING

@article {empiricalForgetting,
   author = {{Goodfellow}, I.~J. and {Mirza}, M. and {Xiao}, D. and {Courville}, A. and 
	{Bengio}, Y.},
    title = "{An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1312.6211},
 primaryClass = "stat.ML",
 keywords = {Statistics - Machine Learning, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing},
     year = 2013,
    month = dec,
   adsurl = {http://adsabs.harvard.edu/abs/2013arXiv1312.6211G},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@ARTICLE{Rubin,
    author = {David C. Rubin and Amy E. Wenzel and David C. Rubin and Department Of Experimental Psychology and Sean Hinton and O Machado and Bennet Murdock and Matt Serra and John Staddon and Wayne Wickelgren and John Wixted For},
    title = {One hundred years of forgetting: A quantitative description of retention},
    journal = {Psychological Review},
    year = {1996},
    pages = {734--760}
}

@ARTICLE{integration,
  
AUTHOR={Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},   
	 
TITLE={Toward an Integration of Deep Learning and Neuroscience},      
	
JOURNAL={Frontiers in Computational Neuroscience},      
	
VOLUME={10},      

PAGES={94},     
	
YEAR={2016},      
	  
URL={http://journal.frontiersin.org/article/10.3389/fncom.2016.00094},       
	
DOI={10.3389/fncom.2016.00094},      
	
ISSN={1662-5188},   
   
ABSTRACT={Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.}
}

%MATHS

@article{Leshno1993,
title = "Multilayer feedforward networks with a nonpolynomial activation function can approximate any function",
journal = "Neural Networks",
volume = "6",
number = "6",
pages = "861 - 867",
year = "1993",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/S0893-6080(05)80131-5",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005801315",
author = "Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken",
keywords = "Multilayer feedforward networks",
keywords = "Activation functions",
keywords = "Role of threshold",
keywords = "Universal approximation capabilities",
}


@article{Hornick1990,
title = "Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks",
journal = "Neural Networks",
volume = "3",
number = "5",
pages = "551 - 560",
year = "1990",
note = "",
issn = "0893-6080",
doi = "http://dx.doi.org/10.1016/0893-6080(90)90005-6",
url = "http://www.sciencedirect.com/science/article/pii/0893608090900056",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
keywords = "Approximation",
keywords = "Derivatives",
keywords = "Sobolev space",
keywords = "Feedforward networks"
}


@article{Hornick1989,
title = "Multilayer feedforward networks are universal approximators",
abstract = "This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.",
keywords = "Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation",
author = "Kurt Hornik and Maxwell Stinchcombe and Halbert White",
year = "1989",
doi = "10.1016/0893-6080(89)90020-8",
volume = "2",
pages = "359--366",
journal = "Neural Networks",
issn = "0893-6080",
publisher = "Elsevier Limited",
number = "5",
}


@ARTICLE{Mhaskar1996,
    author = {H. N. Mhaskar},
    title = {Neural Networks for Optimal Approximation of Smooth and Analytic Functions},
    journal = {Neural Computation},
    year = {1996},
    volume = {8},
    pages = {164--177}
}

@article{meanPeriodicFunctionsSummary,

author = {Székelyhidi, László},

year = {2007},

month = {01},

pages = {69-81},

title = {Mean periodic functions},

volume = {58},

booktitle = {aequationes mathematicae}

}